{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMAR.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jo1jun/Human-Action-Recog-VIBE/blob/main/HMAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHnw7_unEbt"
      },
      "source": [
        "#Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBa2Np7gwPsG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-r8Pe1BnJIU"
      },
      "source": [
        "# Prepare VIBE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo595FintnR9"
      },
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/mkocabas/VIBE.git\n",
        "%cd VIBE/\n",
        "# Install the other requirements\n",
        "!pip install torch==1.4.0 numpy==1.17.5\n",
        "!pip install git+https://github.com/giacaglia/pytube.git --upgrade\n",
        "!pip install -r requirements.txt\n",
        "# Download pretrained weights and SMPL data\n",
        "!source scripts/prepare_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZK0Ll7rxe3O"
      },
      "source": [
        "**Caution**\n",
        "\n",
        "device & torch version must be cuda & 1.4.0 respectively\n",
        "\n",
        "If torch version is not 1.4.0, restart runtime and run from below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_oBXPrPxc2G"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfnrKXT9nUSa"
      },
      "source": [
        "# Custom Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-539u4gbl8d"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "# custom dataset 만들기.\n",
        "class UcfDataset(Dataset):\n",
        "    def __init__(self, base_dir):\n",
        " \n",
        "        # 경로 설정.\n",
        "        with open('/content/drive/MyDrive/HAT/walk_run/walk_run.json') as json_file:\n",
        "            json_data = json.load(json_file)\n",
        "\n",
        "        self.img_names = []\n",
        "        self.annotations = []\n",
        "        self.actions = []\n",
        "        # action_type 0 -> Run-Side, 1 -> Walk-Front\n",
        "        action_type = -1 \n",
        "        for action in list(json_data.keys()):\n",
        "            action_dir = os.path.join(base_dir, action)\n",
        "            action_type += 1\n",
        "            for video_num in list(json_data[action].keys()):\n",
        "                # 아직 완성되지 않은 video는 예외처리.\n",
        "                if len(json_data[action][video_num]) == 0:\n",
        "                    continue\n",
        "\n",
        "                cur_dir = os.path.join(action_dir, video_num)\n",
        "\n",
        "                img_list = json_data[action][video_num]['images']\n",
        "                self.img_names.append([os.path.join(cur_dir, f) for f in img_list])\n",
        "\n",
        "                self.annotations.append(json_data[action][video_num]['bboxes'])\n",
        "\n",
        "                self.actions.append(action_type)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        images = []\n",
        "        print(self.img_names[index]) # image 누락 확인용.\n",
        "        for image_name in self.img_names[index]:\n",
        "          images.append([cv2.imread(image_name)[:,:,::-1].copy().astype(np.float32)])\n",
        "          # images.append([cv2.cvtColor(cv2.imread(image_name), cv2.COLOR_BGR2RGB)])\n",
        "\n",
        "        images = np.array(images).squeeze(1)\n",
        "\n",
        "        return images, np.array(self.annotations[index]), self.actions[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-CnBdeUbati"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset = UcfDataset(\"/content/drive/MyDrive/HAT/walk_run\")\n",
        "dataloader = DataLoader(dataset, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nZ_w0qKoFbD"
      },
      "source": [
        "# Pretrained VIBE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntFX2d9buwib"
      },
      "source": [
        "import os\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "import joblib\n",
        "import shutil\n",
        "import colorsys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from lib.models.vibe import VIBE_Demo\n",
        "from lib.utils.renderer import Renderer\n",
        "from lib.dataset.inference import Inference\n",
        "from lib.utils.smooth_pose import smooth_pose\n",
        "from lib.data_utils.kp_utils import convert_kps\n",
        "from lib.utils.pose_tracker import run_posetracker\n",
        "\n",
        "from lib.utils.demo_utils import (\n",
        "    download_youtube_clip,\n",
        "    smplify_runner,\n",
        "    convert_crop_coords_to_orig_img,\n",
        "    convert_crop_cam_to_orig_img,\n",
        "    prepare_rendering_results,\n",
        "    video_to_images,\n",
        "    images_to_video,\n",
        "    download_ckpt,\n",
        ")\n",
        "\n",
        "MIN_NUM_FRAMES = 16\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# ========= Define VIBE model ========= #\n",
        "model = VIBE_Demo(seqlen=16, n_layers=2, hidden_size=1024,\n",
        "    add_linear=True,\n",
        "    use_residual=True)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# ========= Load pretrained weights ========= #\n",
        "pretrained_file = download_ckpt(use_3dpw=False)\n",
        "ckpt = torch.load(pretrained_file)\n",
        "print(f'Performance of pretrained model on 3DPW: {ckpt[\"performance\"]}')\n",
        "ckpt = ckpt['gen_state_dict']\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model.eval()\n",
        "print(f'Loaded pretrained weights from \\\"{pretrained_file}\\\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLek9C70rJSM"
      },
      "source": [
        "# Classifier (Prototype)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayj17x6irK9P"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, seqlen = 16):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(seqlen * 72 + seqlen * 10, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 1024)\n",
        "    self.fc3 = nn.Linear(1024, 256)\n",
        "    self.fc4 = nn.Linear(256, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, parameters):\n",
        "    x = self.fc1(parameters)\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    out = self.fc4(x)\n",
        "    return self.sigmoid(out)\n",
        "\n",
        "classifier = Classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaGmeOCfraYf"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPH5vgKltarK"
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK9qJbtfrgum"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdpZscu1ri_b"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7VQuk8GoSE6"
      },
      "source": [
        "# Trainer (prototype)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M09aH19wPGr"
      },
      "source": [
        "from lib.data_utils.img_utils import get_single_image_crop_demo\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "bbox_scale = 1.1\n",
        "crop_size = 224\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "classifier.train()\n",
        "\n",
        "for i, (images, bboxes, action) in enumerate(dataloader):\n",
        "\n",
        "  norm_imgs = []\n",
        "\n",
        "  for j, (img, bbox) in enumerate(zip(images.squeeze(0), bboxes.squeeze(0))):\n",
        "\n",
        "    norm_img, raw_img, kp_2d = get_single_image_crop_demo(\n",
        "        img,\n",
        "        bbox,\n",
        "        None,\n",
        "        bbox_scale,\n",
        "        crop_size)\n",
        "    \n",
        "    norm_imgs.append(norm_img.unsqueeze(0))\n",
        "\n",
        "  norm_imgs = torch.cat(norm_imgs)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    norm_imgs = norm_imgs.unsqueeze(0)\n",
        "    num_frames = norm_imgs.shape[1]\n",
        "    norm_imgs = norm_imgs.to(device)\n",
        "\n",
        "    output = model(norm_imgs)[-1]\n",
        "\n",
        "    # classifier 의 input 으로 활용될 VIBE outputs (parameters)\n",
        "    poses = output['theta'][:, :, 3:75].squeeze(0)\n",
        "    betas = output['theta'][:, :, 75:].squeeze(0)\n",
        "    joints3d = output['kp_3d'].squeeze(0)\n",
        "    joints2d = output['kp_2d'].squeeze(0)\n",
        "\n",
        "    # poses : [num_frames, 72]\n",
        "    # betas.shape : [num_frames, 10]\n",
        "    # joints3d : [num_frames, 49, 3]\n",
        "    # joints2d : [num_frames, 49, 2]\n",
        "\n",
        "    cv2_imshow(np.array(norm_imgs[0,0].permute(1,2,0).cpu()))\n",
        "    # 각 폴더 내의 첫번째 image 의 cropped & normalized image 출력.\n",
        "    print('run' if action == 0 else 'walk')\n",
        "\n",
        "  # sequence 를 sampling 해야함. 우선은 가장 처음 sequence 만 사용하는 것으로 설정.\n",
        "  parameters = torch.cat([poses[:16].flatten(), betas[:16].flatten()])\n",
        "  output = classifier(parameters)\n",
        "\n",
        "  action = action.to(device)\n",
        "\n",
        "  action = action.unsqueeze(0)\n",
        "  output = output.unsqueeze(0)\n",
        "  \n",
        "  print(action)\n",
        "  print(output)\n",
        "\n",
        "  loss = criterion(output, action.float())\n",
        "\n",
        "  print(loss)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr1w1ZkW2xmr"
      },
      "source": [
        "# !python demo.py --vid_file /content/drive/MyDrive/HAT/walk_run/Run-Side/009/3687-17_70245.avi --output_folder output/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}